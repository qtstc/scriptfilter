{
 "metadata": {
  "name": "",
  "signature": "sha256:474ee4ce7aa94d8a199f0c0c34889abea0d7d52d46301b3c05bf4e85710cbd0e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import numpy as np\n",
      "import pickle\n",
      "from tld import get_tld\n",
      "from urlparse import urlparse\n",
      "\n",
      "def count_iframe(soup):\n",
      "    return len(soup.find_all('iframe'))\n",
      "\n",
      "def get_domain(url):\n",
      "    try:\n",
      "        domain = get_tld(url)\n",
      "    except:\n",
      "        # Use urlparse if tld does not work\n",
      "        parsed_uri = urlparse(url)\n",
      "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
      "    return domain\n",
      "\n",
      "def count_small_elements(soup):\n",
      "    count = 0\n",
      "    types = ['div','iframe','object']\n",
      "    for t in types:\n",
      "        elements = soup.find_all(t, height = True, width = True)\n",
      "        for e in elements:\n",
      "            attrs = e.attrs\n",
      "            #because sometimes height and width can be percentage\n",
      "            #and we just ignore this\n",
      "            try:\n",
      "                h = int(attrs['height'])\n",
      "                w = int(attrs['width'])\n",
      "                # Here is the rule\n",
      "                if h <= 2 or w <= 2:\n",
      "                    count += 1\n",
      "                    continue\n",
      "                if h * w <= 30:\n",
      "                    count += 1\n",
      "            except:\n",
      "                continue\n",
      "    return count\n",
      "\n",
      "def count_script(soup):\n",
      "    return len(soup.find_all('script'))\n",
      "\n",
      "def get_script_wrong_extension(soup):\n",
      "    elements = soup.find_all('script', src = True)\n",
      "    count = 0\n",
      "    for e in elements:\n",
      "        url = e.attrs['src']\n",
      "        if not url.endswith('.js'):\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "def get_script_len(soup):\n",
      "    script_len = 0\n",
      "    for el in soup.find_all('script'):\n",
      "        script_len += len(str(el))\n",
      "    return script_len\n",
      "\n",
      "def count_whitespace(soup):\n",
      "    return str(soup).count(' ')\n",
      "\n",
      "def count_meta_refresh(soup):\n",
      "    # look for things of the form   <meta http-equiv=\"refresh\" content=\"5\">\n",
      "    elements = soup.find_all('meta',attrs={\"http-equiv\": \"refresh\"})\n",
      "    return len(elements)\n",
      "\n",
      "def count_embed_and_object(soup):\n",
      "    return len(soup.find_all('embed')) + len(soup.find_all('object'))\n",
      "\n",
      "def get_src_external_domain(soup, domain):\n",
      "    elements = soup.find_all('script', src = True)\n",
      "    count = 0\n",
      "    for e in elements:\n",
      "        url = e.attrs['src']\n",
      "        # Here we just check whether the domain is a substring of src\n",
      "        if not domain in url:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "def get_included_element_count(soup):\n",
      "    types = ['script', 'iframe', 'frame', 'embed', 'form', 'object']\n",
      "    count = 0\n",
      "    for t in types:\n",
      "        elements = soup.find_all(t)\n",
      "        count += len(elements)\n",
      "    return count\n",
      "\n",
      "def count_double_documents(soup):\n",
      "    types = ['html', 'head', 'title', 'body']\n",
      "    count = 0\n",
      "    for t in types:\n",
      "        if len(soup.find_all(t)) > 1:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "def get_char_count(soup):\n",
      "    return len(str(soup))\n",
      "\n",
      "def get_features(request):\n",
      "    a = []\n",
      "    soup = BeautifulSoup(request.content)\n",
      "    domain = get_domain(request.url)\n",
      "    a.append(count_iframe(soup))\n",
      "    a.append(count_small_elements(soup))\n",
      "    a.append(count_script(soup))\n",
      "    a.append(get_script_wrong_extension(soup))\n",
      "    a.append(get_script_len(soup))\n",
      "    a.append(count_whitespace(soup))\n",
      "    a.append(count_meta_refresh(soup))\n",
      "    a.append(count_embed_and_object(soup))\n",
      "    a.append(get_src_external_domain(soup, domain))\n",
      "    a.append(get_included_element_count(soup))\n",
      "    a.append(count_double_documents(soup))\n",
      "    a.append(get_char_count(soup))\n",
      "    return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "with open('data/unique_negative_pages.pkl','r') as f:\n",
      "    negative_pages = pickle.load(f)\n",
      "with open('data/unique_positive_pages.pkl','r') as f:\n",
      "    positive_pages = pickle.load(f)\n",
      "\n",
      "data = np.zeros([1500, 12], dtype=float)\n",
      "target = np.zeros([1500,1], dtype=int)\n",
      "index = 0\n",
      "for positive_page in positive_pages:\n",
      "    vector = get_features(positive_page)\n",
      "    vector.append(1)\n",
      "    data[index] = vector\n",
      "    index += 1\n",
      "    \n",
      "neg_page_index = 0\n",
      "while index < len(data):\n",
      "    vector = get_features(negative_pages[neg_page_index])\n",
      "    vector.append(0)\n",
      "    data[index] = vector\n",
      "    index += 1\n",
      "    neg_page_index += 1\n",
      "\n",
      "data.dump(\"labeled_data.dat\")\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.load(\"labeled_data.dat\")\n",
      "np.random.shuffle(data)\n",
      "training = data[0:1000]\n",
      "testing = data[1000:]\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "#clf = svm.SVC(gamma=0.001, C=100.)\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf.fit(training[:,:-1], training[:,-1])  \n",
      "false_positive = 0\n",
      "false_negative = 0\n",
      "true_positive = 0\n",
      "true_negative = 0\n",
      "for i in range(0,len(testing)):\n",
      "    predicted = clf.predict(testing[i,:-1])\n",
      "    if predicted != testing[i,-1]:\n",
      "        if predicted == 1:\n",
      "            false_positive += 1\n",
      "        else:\n",
      "            false_negative += 1\n",
      "    else:\n",
      "        if predicted == 1:\n",
      "            true_positive += 1\n",
      "        else:\n",
      "            true_negative += 1\n",
      "print str(false_positive) +\" vs. \"+ str(true_positive)\n",
      "print str(false_negative) + \" vs. \" + str(true_negative)\n",
      "print \"Accuracy: \" + str((true_positive+true_negative)/float(len(testing)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "67 vs. 209\n",
        "45 vs. 179\n",
        "Accuracy: 0.776\n"
       ]
      }
     ],
     "prompt_number": 90
    }
   ],
   "metadata": {}
  }
 ]
}