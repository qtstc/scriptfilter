{
 "metadata": {
  "name": "",
  "signature": "sha256:0e2b7042143e23da49c02351d8e2ea1679daa01e10482189e59c037ce99e467c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "import pickle\n",
      "\n",
      "# Parse websites from malicious_edited.txt\n",
      "f = open('malicious_edited.txt')\n",
      "line = f.readline()\n",
      "prefix = '0.0.0.0 '\n",
      "prefix_len = len(prefix)\n",
      "positive_list = []\n",
      "while line:\n",
      "    if not line.startswith(prefix):\n",
      "        line = f.readline()\n",
      "        continue\n",
      "    line = line[prefix_len:len(line)-1]\n",
      "    space_loc = line.find(\" \")\n",
      "    if space_loc != -1:\n",
      "        line=line[0:space_loc]\n",
      "    positive_list.append(line)\n",
      "    line = f.readline()\n",
      "f.close()\n",
      "pickle.dump(positive_list, open('positive.pkl', 'wb')) \n",
      "\n",
      "# Parse websites from Alex's top 1 million list\n",
      "f = open('top-1m.csv')\n",
      "line = f.readline()\n",
      "count = 0\n",
      "negative_list = []\n",
      "while line and count < 3000:\n",
      "    comma_loc = line.find(\",\")\n",
      "    if comma_loc != -1:\n",
      "        line=line[comma_loc+1:len(line)-1]\n",
      "    else:\n",
      "        print \"Wrong format\"\n",
      "    negative_list.append(line)\n",
      "    line = f.readline()\n",
      "    count += 1\n",
      "f.close()\n",
      "pickle.dump(negative_list, open('negative.pkl','wb'))\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "import pickle\n",
      "import requests\n",
      "\n",
      "# Download pages from the list of url\n",
      "def download_pages(file_folder_prefix, url_list,batch_size = 200):\n",
      "    pages = []\n",
      "    failed_list = []\n",
      "    file_count = 0\n",
      "    for url in url_list:\n",
      "        result = None\n",
      "        try:\n",
      "            result = requests.get('http://'+url, allow_redirects=True,timeout=5)\n",
      "        except:\n",
      "            print \"ERROR:\"+url\n",
      "            failed_list.append(url)\n",
      "        pages.append((url,result))\n",
      "        if(len(pages) == batch_size):\n",
      "            with open(file_folder_prefix+'_pages_'+str(file_count)+'.pkl','w') as f:\n",
      "                pickle.dump(pages, f)\n",
      "            file_count += 1\n",
      "            print \"Finished processing \"+str(batch_size)\n",
      "            pages = []\n",
      "\n",
      "    if len(pages) != 0:\n",
      "        with open(file_folder_prefix+'_pages_'+str(file_count)+'.pkl','w') as f:\n",
      "            pickle.dump(pages, f)\n",
      "\n",
      "    with open(file_folder_prefix+'_failed.pkl','w') as f:\n",
      "        pickle.dump(failed_list, f)\n",
      "    print \"Finished \"+str(len(failed_list))+\" failed\"\n",
      "    \n",
      "with open('negative.pkl','r') as f:\n",
      "    negative_list = pickle.load(f)\n",
      "with open('positive.pkl','r') as f:\n",
      "    positive_list = pickle.load(f)\n",
      "\n",
      "print \"There are \"+str(len(negative_list))+ \" neg samples\"\n",
      "print \"There are \"+str(len(positive_list))+ \" pos samples\"\n",
      "download_pages('pages/positive',positive_list)\n",
      "download_pages('pages/negative',negative_list)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 3000 neg samples\n",
        "There are 2459 pos samples\n",
        "ERROR:soso.com"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ERROR:googleusercontent.com"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ERROR:thepiratebay.se"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "import pickle\n",
      "import requests\n",
      "\n",
      "\n",
      "# Retry download for the ones that are failed.\n",
      "def retry_failed_urls(file_folder_prefix,file_count):\n",
      "    for i in range(0,file_count):\n",
      "        file_name = file_folder_prefix+\"_pages_\"+str(i)+\".pkl\"\n",
      "        print \"Processing: \"+file_name\n",
      "        total_missed_count = 0\n",
      "        updated_count = 0\n",
      "        new_pages = []\n",
      "        with open(file_name,'r') as f:\n",
      "            pages = None\n",
      "            pages = pickle.load(f)\n",
      "            for url,request in pages:\n",
      "                new_request = reques\n",
      "                if new_request == None:\n",
      "                    total_missed_count += 1\n",
      "                    try:\n",
      "                        #Here we just attemp to add www to the url\n",
      "                        #but more things can be tried here\n",
      "                        new_request = requests.get('http://www.'+url, allow_redirects=True,timeout=5)\n",
      "                    except:\n",
      "                        new_request = None\n",
      "                    if new_request != None:\n",
      "                        print \"Fixed \"+url\n",
      "                        updated_count += 1\n",
      "                new_pages.append((url,new_request))\n",
      "        if updated_count > 0:\n",
      "            with open(file_name,'w') as f:\n",
      "                pickle.dump(new_pages, f)\n",
      "        print \"Finished processing \"+file_name\n",
      "        print \"Original missed count is \"+str(total_missed_count)\n",
      "        print \"Fixed \"+str(updated_count)\n",
      "        \n",
      "retry_failed_urls('pages/positive',13)\n",
      "retry_failed_urls('pages/negative',15)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "import requests\n",
      "import pickle\n",
      "import hashlib\n",
      "\n",
      "# Remove duplicates in the webpages based on a hash of the content of the web page\n",
      "# Here we used the md5 hash of hashlib. Ideally, a locality sensitive hashing \n",
      "# is more effective in removing duplicated webpages.\n",
      "def remove_duplicate(file_folder_prefix,result_file_name,file_count):\n",
      "    page_dict = {}\n",
      "    duplicate_count = 0\n",
      "    for i in range(0,file_count):\n",
      "        file_name = file_folder_prefix+\"_pages_\"+str(i)+\".pkl\"\n",
      "        with open(file_name,'r') as f:\n",
      "            pages = pickle.load(f)\n",
      "            for (url, result) in pages:\n",
      "                if result is None:\n",
      "                    continue\n",
      "                page_hash = hashlib.md5(result.content).hexdigest()\n",
      "                if page_hash not in page_dict:\n",
      "                    page_dict[page_hash] = result\n",
      "                else:\n",
      "                    duplicate_count += 1\n",
      "        print \"Finished processing \"+file_name\n",
      "\n",
      "    print \"Removed \"+str(duplicate_count)+\" duplicates\"\n",
      "    with open(result_file_name,'w') as f:\n",
      "        pickle.dump(page_dict.values(),f)\n",
      "\n",
      "    print \"Saved \"+str(len(page_dict.values()))+\" results to \"+result_file_name\n",
      "\n",
      "remove_duplicate('pages/positive','unique_positive_pages.pkl',13)\n",
      "remove_duplicate('pages/negative','unique_negative_pages.pkl',15)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished processing pages/negative_pages_0.pkl\n",
        "Finished processing pages/negative_pages_1.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_2.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_3.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_4.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_5.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_6.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_7.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_8.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_9.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_10.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_11.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_12.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_13.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished processing pages/negative_pages_14.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Removed 51 duplicates\n",
        "Saved 2881 results to unique_negative_pages.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    }
   ],
   "metadata": {}
  }
 ]
}